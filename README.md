# <p align='center'>`Advances in 3D Neural Stylization`</p>

A review of 3D neural stylization papers, mainly neural stylization on 3D data with image or text reference.

Other variants are also welcomed, e.g., with other/no reference, 3D-aware neural stylization, non-neural 3D stylization.

## Contributing

If you think I have missed out on something (or) have any suggestions (papers, implementations and other resources), feel free to [pull a request](https://github.com/chenyingshu/advances_3d_neural_stylization/pulls)

Feedback and contributions are welcome!

markdown format:
``` markdown
**Here is the Paper Name.**<br>
*[Author 1](homepage), Author 2, and Author 3.*<br>
Conference or Journal Year. <br>
[[Paper](link)] [[Project](link)] [[Github](link)] [[Video](link)] [[Data](link)] <br>
_Features_:
- highlight feature 1
- highlight feature 2
```

## Surveys
**Advances in 3D Neural Stylization: A Survey** <br>
_Yingshu Chen, Guocheng Shao, Ka Chun Shum, Binh-Son Hua, Sai-Kit Yeung._ IJCV 2025.<br/><br/>
<a href="http://www.chenyingshu.com/advances_3d_neural_stylization/"><img src="https://img.shields.io/badge/WEBSITE-Access%20Here-blue?style=for-the-badge"></a>
<a href="https://arxiv.org/abs/2311.18328"><img src="https://img.shields.io/badge/arxiv-2311.18328-red?style=for-the-badge"></a>
<a href="https://github.com/chenyingshu/advances_3d_neural_stylization"><img src="https://img.shields.io/badge/CODE-Access%20Github-0366d6?style=for-the-badge"></a>

### Citation
😊 If you find this work useful, please cite our paper:
```bibtex
@article{chen2025advances,
  title={Advances in 3D neural stylization: a survey},
  author={Chen, Yingshu and Shao, Guocheng and Shum, Ka Chun and Hua, Binh-Son and Yeung, Sai-Kit},
  journal={International Journal of Computer Vision},
  pages={1--36},
  year={2025},
  publisher={Springer}
}
```

### Acknowledgement
The paper was partially supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. HKUST 16202323) and an internal grant from HKUST (R9429).<br/>
Binh-Son Hua is supported by Research Ireland under the Research Ireland Frontiers for the Future Programme - Project, award number 22/FFP-P/11522.

<details>
  <summary>
    
### Other related surveys, courses
  </summary>
  
**Neural Style Transfer: A Review** [[Paper](https://arxiv.org/abs/1705.04058)] [[Project](https://github.com/ycjing/Neural-Style-Transfer-Papers)] <br>
*Yongcheng Jing, Yezhou Yang, Zunlei Feng, Jingwen Ye, Yizhou Yu, Mingli Song.* TVCG, 2019.

**Multimodal Image Synthesis and Editing: The Generative AI Era** [[Paper](https://arxiv.org/abs/2112.13592)]  [[Project](https://fnzhan.com/Generative-AI/)]  <br>
*Fangneng Zhan, Yingchen Yu, Rongliang Wu, Jiahui Zhang, Shijian Lu, Lingjie Liu, Adam Kortylewski, Christian Theobalt, Eric Xing.* TPAMI, 2023.

**Diffusion Models in Vision: A Survey** [[Paper](https://arxiv.org/abs/2209.04747)] [[Project](https://github.com/CroitoruAlin/Diffusion-Models-in-Vision-A-Survey)] <br>
*Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Mubarak Shah.* TPAMI, 2023.

**Diffusion Models: A Comprehensive Survey of Methods and Applications** [[Paper](https://arxiv.org/abs/2209.00796)][[Project](https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy)] <br>
*Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, Ming-Hsuan Yang.* ACM Computing Surveys 2023.

**State of the Art on Diffusion Models for Visual Computing** [[Paper](https://arxiv.org/abs/2310.07204)] [[Project](https://web.stanford.edu/~gordonwz/diffusion/)] <br>
*Ryan Po, Yifan Wang, Vladislav Golyanik, Kfir Aberman, Jon T. Barron, Amit Bermano, Eric Chan, Tali Dekel, Aleksander Holynski, Angjoo Kanazawa, C. Karen Liu, Lingjie Liu, Ben Mildenhall, Matthias Niessner, Björn Ommer, Christian Theobalt, Peter Wonka, and Gordon Wetzstein.* State of the Art Report at EUROGRAPHICS 2024.

**State of the Art on Neural Rendering** [[Paper](https://arxiv.org/abs/2004.03805)] <br>
*Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann, Stephen Lombardi, Kalyan Sunkavalli, Ricardo Martin-Brualla, Tomas Simon, Jason Saragih, Matthias Nießner, Rohit Pandey, Sean Fanello, Gordon Wetzstein, Jun-Yan Zhu, Christian Theobalt, Maneesh Agrawala, Eli Shechtman, Dan B Goldman, Michael Zollhöfer.* State of the Art Report at EUROGRAPHICS 2020. <br>
**Neural Rendering** [[Project](https://www.neuralrender.com/CVPR/)] CVPR 2020 Tutorial. <br>
**Advances in Neural Rendering** [[Paper](https://dl.acm.org/doi/10.1145/3450508.3464573)] [[Project](https://www.neuralrender.com/)] SIGGRAPH 2021 Courses.


**Advances in Neural Rendering** [[Paper](https://arxiv.org/abs/2111.05849)] <br>
*Ayush Tewari, Justus Thies, Ben Mildenhall, Pratul Srinivasan, Edgar Tretschk, Yifan Wang, Christoph Lassner, Vincent Sitzmann, Ricardo Martin-Brualla, Stephen Lombardi, Tomas Simon, Christian Theobalt, Matthias Niessner, Jonathan T. Barron, Gordon Wetzstein, Michael Zollhoefer, Vladislav Golyanik.* State of the Art Report at EUROGRAPHICS 2022.

**Neural Fields in Visual Computing and Beyond** [[Paper](https://arxiv.org/abs/2111.11426)]  [[Project](https://neuralfields.cs.brown.edu/)]  <br>
*Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico Tombari, James Tompkin, Vincent Sitzmann, Srinath Sridhar.* State of the Art Report at EUROGRAPHICS 2022. <br>
**Neural Fields for Visual Computing** [[Paper](https://dl.acm.org/doi/abs/10.1145/3587423.3595477)] [[Project](https://neuralfields.cs.brown.edu/siggraph23.html)] <br>
*Towaki Takikawa, Shunsuke Saito, James Tompkin, Vincent Sitzmann, Srinath Sridhar, Or Litany, Alex Yu.* SIGGRAPH 2023 Courses.<br>

**Generative AI meets 3D: A Survey on Text-to-3D in AIGC Era** [[Paper](https://arxiv.org/abs/2305.06131)]<br>
*Chenghao Li, Chaoning Zhang, Atish Waghwase, Lik-Hang Lee, Francois Rameau, Yang Yang, Sung-Ho Bae, Choong Seon Hong*<br>

**Text-to-3D Shape Generation** [[Paper](https://arxiv.org/abs/2403.13289)] [[Project](https://3dlg-hcvc.github.io/tt3dstar/)] <br>
*Han-Hung Lee, Manolis Savva, and Angel Xuan Chang.* State of the Art Report at EUROGRAPHICS 2024.

**3D Gaussian Splatting as New Era: A Survey** [[Paper](https://arxiv.org/abs/2402.07181)] <br>
*Ben Fei, Jingyi Xu, Rui Zhang, Qingyuan Zhou, Weidong Yang, Ying He.* TVCG 2024.

</details>

## Table of Contents
- [Mesh Stylization](#table-mesh-style) [(Detail)](#mesh-stylization)
- [Neural Field Stylization](#table-novel-view-style) [(Detail)](#neural-field-stylization)
- [Multi-view Stylization](#table-light-field-style)
- [Point Cloud Stylization](#table-point-style) [(Detail)](#point-cloud-stylization)
- [Volumetric Stylization](#table-volume-style) [(Detail)](#volumetric-stylization)
- [Implicit Shape Stylization](#table-implicit-shape-style)
- [Other 3D Stylization](#other-3d-stylization)
- [Related 3D Generation](#other-related-3d-generation)
- [Evaluation](https://github.com/chenyingshu/advances_3d_neural_stylization/tree/main/evaluation_codes)


<details open>
  <summary>
    
## Overview of 3D Neural Stylization ##
  </summary>
<div align=left> 
  Image/Video guidance: <img align=top src="assets/icon_image.png" width="32" height="32"> 
  Text guidance: <img align=top src="assets/icon_text.png" width="32" height="32"> 
  3D/4D guidance: <img align=top src="assets/icon_obj.png" width="32" height="32"> 
</div>

<details open>
<summary>

**Mesh Stylization** <div id="table-mesh-style"></div>

</summary>

|  Abbr.  |  Title   | Venue  | Paper  | Project  | Github  |
|  ----  |  ----    | ----  |----   |----  |----  |
|Geometry in Style <img src="assets/icon_text.png" width="20" height="20">|Geometry in Style: 3D Stylization via Surface Normal Deformation | CVPR 2025|[[Paper](https://arxiv.org/abs/2503.23241)] |[[Project](https://threedle.github.io/geometry-in-style/)]| [[Github](https://github.com/threedle/geometry-in-style/)]|
|MeshUp <img src="assets/icon_text.png" width="20" height="20">|MeshUp: Multi-Target Mesh Deformation via Blended Score Distillation | 3DV 2025(Honorable Mention) | [[Paper](https://arxiv.org/abs/2408.14899)] | [[Project](https://threedle.github.io/MeshUp/)] | [[Github](https://github.com/threedle/MeshUp)] |
|FlashTex <img src="assets/icon_text.png" width="20" height="20">| FlashTex: Fast Relightable Mesh Texturing with LightControlNet | ECCV 2024(Oral) |[[Paper](https://arxiv.org/abs/2402.13251)]|[[Project](https://flashtex.github.io/)]|[[Github](https://github.com/Roblox/FlashTex)]|
|TexGen <img src="assets/icon_text.png" width="20" height="20">|TexGen: Text-Guided 3D Texture Generation with Multi-view Sampling and Resampling| ECCV 2024 |[[Paper](https://arxiv.org/abs/2408.01291)]| [[Project](https://dong-huo.github.io/TexGen/)]|
|TexDreamer <img src="assets/icon_image.png" width="20" height="20"><img src="assets/icon_text.png" width="20" height="20">|TexDreamer: Towards Zero-Shot High-Fidelity 3D Human Texture Generation| ECCV 2024 | [[Paper](https://arxiv.org/abs/2403.12906)]| [[Project](https://ggxxii.github.io/texdreamer/)]| [[Github](https://github.com/ggxxii/texdreamer)]|
|Scene-Conditional 3D <img src="assets/icon_image.png" width="20" height="20"> |Scene-Conditional 3D Object Stylization and Composition |ECCV 2024| [[Paper](https://arxiv.org/abs/2312.12419)]| [[Project](https://shallowtoil.github.io/scene-cond-3d/)]||
|VCD-Texture <img src="assets/icon_text.png" width="20" height="20"> |VCD-Texture: Variance Alignment based 3D-2D Co-Denoising for Text-Guided Texturing| ECCV 2024| [[Paper](https://arxiv.org/abs/2407.04461)]|||
|MMDMC <img align=top src="assets/icon_obj.png" width="32" height="32">  |Towards High-Quality 3D Motion Transfer with Realistic Apparel Animation|ECCV 2024 |[[Paper](https://arxiv.org/abs/2407.11266)]||[[Data](https://github.com/rongakowang/MMDMC)]|
|StyleCity <img src="assets/icon_image.png" width="20" height="20"><img src="assets/icon_text.png" width="20" height="20"> |StyleCity: Large-Scale 3D Urban Scenes Stylization | ECCV 2024| [[Paper](https://arxiv.org/abs/2404.10681)]|[[Project](https://chenyingshu.github.io/stylecity3d/)]| [[Github(TBA)](https://github.com/chenyingshu/stylecity3d)]|
| Img2LEGO <img src="assets/icon_image.png" width="20" height="20"> |Creating LEGO Figurines From Single Images|SIGGRAPH 2024| [[Paper](https://dl.acm.org/doi/10.1145/3658167)]| | |
|CNSTDM <img src="assets/icon_image.png" width="20" height="20">|Controllable Neural Style Transfer for Dynamic Meshes|SIGGRAPH 2024| [[Paper](https://dl.acm.org/doi/10.1145/3641519.3657474)]|||
|HeadArtist  <img src="assets/icon_text.png" width="20" height="20">| HeadArtist: Text-conditioned 3D Head Generation with Self Score Distillation| SIGGRAPH 2024 | [[Paper](https://arxiv.org/abs/2312.07539)]| [[Project](https://kumapowerliu.github.io/HeadArtist/)] | [[Github](https://github.com/KumapowerLIU/HeadArtist)]|
|MaPa <img src="assets/icon_text.png" width="20" height="20">|MaPa: Text-driven Photorealistic Material Painting for 3D Shapes| SIGGRAPH 2024 Conf Paper |[[Paper](https://arxiv.org/abs/2404.17569)]|[[Project](https://zhanghe3z.github.io/MaPa/)]||
|DTP <img src="assets/icon_image.png" width="20" height="20">|Diffusion Texture Painting|SIGGRAPH 2024 Conf Paper|[[Paper](https://dl.acm.org/doi/10.1145/3641519.3657458)]|[[Project](https://research.nvidia.com/labs/toronto-ai/DiffusionTexturePainting/)]| [[Github](https://github.com/nv-tlabs/DiffusionTexturePainting)]|
|TexPainter <img src="assets/icon_text.png" width="20" height="20">|TexPainter: Generative Mesh Texturing With Multi-view Consistency|SIGGRAPH 2024 Conf Paper| [[Paper](https://arxiv.org/abs/2406.18539)]| [[Project](https://quantuman134.github.io/)]| [[Github](https://github.com/Quantuman134/TexPainter)]|
|DreamMat <img src="assets/icon_text.png" width="20" height="20">|DreamMat: High-quality PBR Material Generation With Geometry- and Light-aware Diffusion Models|SIGGRAPH/TOG 2024| [[Paper](https://arxiv.org/abs/2405.17176)]| [[Project](https://zzzyuqing.github.io/dreammat.github.io/)]|[[Github](https://github.com/zzzyuqing/DreamMat)]|
|EASI-Tex <img src="assets/icon_image.png" width="20" height="20">|EASI-Tex: Edge-Aware Mesh Texturing from Single Image|SIGGRAPH/TOG 2024|[[Paper](https://arxiv.org/abs/2405.17393)]|[[Project](https://sairajk.github.io/easi-tex/)]|[[Github](https://github.com/sairajk/easi-tex)]|
|Coin3D <img src="assets/icon_text.png" width="20" height="20">|Coin3D: Controllable and Interactive 3D Assets Generation With Proxy-guided Conditioning|SIGGRAPH 2024 Conf Paper|[[Paper](https://arxiv.org/abs/2405.08054)]|[[Project](https://zju3dv.github.io/coin3d/)]|[[Github(TBA)](https://github.com/zju3dv/Coin3D)]|
|ThemeStation <img src="assets/icon_image.png" width="20" height="20"><img src="assets/icon_obj.png" width="20" height="20">|ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars|SIGGRAPH 2024 Conf Paper|[[Paper](https://arxiv.org/abs/2403.15383)]|[[Project](https://3dthemestation.github.io/)]|[[Github](https://github.com/3DTopia/ThemeStation)]|
|DressCode <img src="assets/icon_text.png" width="20" height="20"> |DressCode: Autoregressively Sewing and Generating Garments from Text Guidance|SIGGRAPH/TOG 2024|[[Paper](https://arxiv.org/abs/2401.16465)]|[[Project](https://ihe-kaii.github.io/DressCode/)]|[[Github](https://github.com/IHe-KaiI/DressCode)]|
|MeshNCA <img src="assets/icon_image.png" width="20" height="20"><img src="assets/icon_text.png" width="20" height="20">|Mesh Neural Cellular Automata|SIGGRAPH/TOG 2024| [[Paper](https://arxiv.org/abs/2311.02820)]|[[Project](https://meshnca.github.io/)]|[[Github](https://github.com/IVRL/MeshNCA)]|
|Paint3D <img src="assets/icon_text.png" width="20" height="20"><img src="assets/icon_image.png" width="20" height="20">|Paint3D: Paint Anything 3D with Lighting-Less Texture Diffusion Models| CVPR 2024| [[Paper](https://arxiv.org/abs/2312.13913)]|[[Project](https://paint3d.github.io/)] |[[Github](https://github.com/OpenTexture/Paint3D)]|
|TextureDreamer <img src="assets/icon_image.png" width="20" height="20">|TextureDreamer: Image-guided Texture Synthesis through Geometry-aware Diffusion | CVPR 2024| [[Paper](https://arxiv.org/abs/2401.09416)] | [[Project](https://texturedreamer.github.io/)]| |
|TeMO <img src="assets/icon_text.png" width="20" height="20">|TeMO: Towards Text-Driven 3D Stylization for Multi-Object Meshes |CVPR 2024 | [[Paper](https://arxiv.org/abs/2312.04248)]||[[Github(TBA)](https://github.com/zhangxuying1004/TeMO)]|
|3DPaintbrush <img src="assets/icon_text.png" width="20" height="20"> |3D Paintbrush: Local Stylization of 3D Shapes with Cascaded Score Distillation| CVPR 2024|[[Paper](https://arxiv.org/abs/2311.09571)]|[[Project](https://threedle.github.io/3d-paintbrush/)]|[[Github](https://github.com/threedle/3d-paintbrush)]|
|As-Plausible-As-Possible|As-Plausible-As-Possible: Plausibility-Aware Mesh Deformation Using 2D Diffusion Priors|CVPR 2024|[[Paper](https://as-plausible-as-possible.github.io/static/APAP.pdf)]| [[Project](https://as-plausible-as-possible.github.io/)]| [[Github](https://github.com/KAIST-Visual-AI-Group/APAP)]|
|SceneTex<img src="assets/icon_text.png" width="20" height="20"> | SceneTex: High-Quality Texture Synthesis for Indoor Scenes via Diffusion Priors |CVPR 2024| [[Paper](https://arxiv.org/abs/2311.17261)]| [[Project](https://daveredrum.github.io/SceneTex/)]| [[Github](https://github.com/daveredrum/SceneTex)]|
|Paint-it <img src="assets/icon_text.png" width="20" height="20">  | Paint-it: Text-to-Texture Synthesis via Deep Convolutional Texture Map Optimization and Physically-Based Rendering| CVPR 2024| [[Paper](https://kim-youwang.github.io/media/paint-it/paint-it.pdf)]|[[Project](https://kim-youwang.github.io/paint-it)] | [[Github](https://github.com/postech-ami/paint-it)] |
| DreamSpace <img src="assets/icon_text.png" width="20" height="20"> | Dreaming Your Room Space with Text-Driven Panoramic Texture Propagation| IEEE VR 2024| [[Paper](https://ybbbbt.com/publication/dreamspace/media/DreamSpace.pdf)] | [[Project](https://ybbbbt.com/publication/dreamspace/)] | [[Github](https://github.com/ybbbbt/dreamspace)] |
|TECA <img src="assets/icon_text.png" width="20" height="20">|TECA: Text-Guided Generation and Editing of Compositional 3D Avatars |3DV 2024 |[[Paper](https://arxiv.org/abs/2309.07125)]|[[Project](https://yfeng95.github.io/teca/)]|[[Github](https://github.com/HaoZhang990127/TECA)] |
|StyleAvatar <img src="assets/icon_text.png" width="20" height="20"><img src="assets/icon_image.png" width="20" height="20">|StyleAvatar: Stylizing Animatable Head Avatars| WACV 2024|[[Paper](https://openaccess.thecvf.com/content/WACV2024/papers/Perez_StyleAvatar_Stylizing_Animatable_Head_Avatars_WACV_2024_paper.pdf)]|||
| Text2Scene <img src="assets/icon_image.png" width="20" height="20"><img src="assets/icon_text.png" width="20" height="20">| Text2Scene: Text-Driven Indoor Scene Stylization With Part-Aware Details  |CVPR 2023|[[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Hwang_Text2Scene_Text-Driven_Indoor_Scene_Stylization_With_Part-Aware_Details_CVPR_2023_paper.html)]| [[Video](https://www.youtube.com/watch?v=CGIXY2kwIYM)] |  |
| TextDeformer <img src="assets/icon_text.png" width="20" height="20">| TextDeformer: Geometry Manipulation using Text Guidance | SIGGRAPH 2023 Conf Paper| [[Paper](https://arxiv.org/abs/2304.13348)] | [[Project](https://threedle.github.io/TextDeformer/)] | [[Github](https://github.com/threedle/TextDeformer)] |
| TEXTure <img src="assets/icon_text.png" width="20" height="20"><img src="assets/icon_image.png" width="20" height="20">| TEXTure: Text-Guided Texturing of 3D Shapes | SIGGRAPH 2023 Conf Paper | [[Paper](https://arxiv.org/abs/2302.01721)] | [[Project](https://texturepaper.github.io/TEXTurePaper/)] | [[Github](https://github.com/TEXTurePaper/TEXTurePaper)] |
| Creative Birds <img src="assets/icon_text.png" width="20" height="20">| Creative Birds: Self-Supervised Single-View 3D Style Transfer | ICCV 2023 | [[Paper](https://arxiv.org/abs/2307.14127)] | | [[Github](https://github.com/wrk226/creative_birds)] |
| Text2Tex <img src="assets/icon_text.png" width="20" height="20">| Text2Tex: Text-driven Texture Synthesis via Diffusion Models | ICCV 2023 | [[Paper](https://daveredrum.github.io/Text2Tex/static/Text2Tex.pdf)] | [[Project](https://daveredrum.github.io/Text2Tex/)] | [[Github](https://github.com/daveredrum/Text2Tex)] |
|TexFusion <img src="assets/icon_text.png" width="20" height="20">| TexFusion: Synthesizing 3D Textures with Text-Guided Image Diffusion Models| ICCV 2023 |[[Paper](https://openaccess.thecvf.com//content/ICCV2023/papers/Cao_TexFusion_Synthesizing_3D_Textures_with_Text-Guided_Image_Diffusion_Models_ICCV_2023_paper.pdf)] | [[Project](https://research.nvidia.com/labs/toronto-ai/publication/2023_iccv_texfusion/)]|  | 
| X-Mesh <img src="assets/icon_text.png" width="20" height="20">| X-Mesh:Towards Fast and Accurate Text-driven 3D Stylization via Dynamic Textual Guidance | ICCV 2023 | [[Paper](https://arxiv.org/abs/2303.15764)] | [[Project](https://xmu-xiaoma666.github.io/Projects/X-Mesh/)] | [[Github](https://github.com/xmu-xiaoma666/X-Mesh)] |
|Point-UV Diffusion  <img src="assets/icon_text.png" width="20" height="20"><img src="assets/icon_image.png" width="20" height="20"> |Texture Generation on 3D Meshes with Point-UV Diffusion | ICCV 2023| [[Paper](https://cvmi-lab.github.io/Point-UV-Diffusion/paper/point_uv_diffusion.pdf)] |[[Project](https://cvmi-lab.github.io/Point-UV-Diffusion/)] | [[Github](https://github.com/CVMI-Lab/Point-UV-Diffusion)]|
|3DStyle-Diffusion<img src="assets/icon_text.png" width="20" height="20"> | 3DStyle-Diffusion: Pursuing Fine-grained Text-driven 3D Stylization with 2D Diffusion Models | ACM MM 2023| [[Paper](https://arxiv.org/abs/2311.05464)] | | [[Github](https://github.com/yanghb22-fdu/3dstyle-diffusion-official)] |
|RoomDreamer<img src="assets/icon_text.png" width="20" height="20"> | RoomDreamer: Text-Driven 3D Indoor Scene Synthesis with Coherent Geometry and Texture |ACM MM 2023| [[Paper](https://arxiv.org/abs/2305.11337)]| [[Video](https://youtu.be/p4xgwj4QJcQ)] | |
| Decorate3D <img src="assets/icon_text.png" width="20" height="20"> | Decorate3D: Text-Driven High-Quality Texture Generation for Mesh Decoration in the Wild| NeurIPS 2023| [[Paper](https://decorate3d.github.io/Decorate3D/static/Decorate3D.pdf)] | [[Project](https://decorate3d.github.io/Decorate3D/)] | [[Github](https://github.com/Decorate3D/Decorate3D)] |
|HyperDreamer <img src="assets/icon_text.png" width="20" height="20">|HyperDreamer: Hyper-Realistic 3D Content Generation and Editing from a Single Image |SIGGRAPH Asia 2023 Conf Paper |[[Paper](https://arxiv.org/abs/2312.04543)] |[[Project](https://ys-imtech.github.io/HyperDreamer/)]| [[Github](https://github.com/wutong16/HyperDreamer)]|
|AlteredAvatar <img src="assets/icon_text.png" width="20" height="20">|AlteredAvatar: Stylizing Dynamic 3D Avatars with Fast Style Adaptation|Arxiv 2023| [[Paper](https://arxiv.org/abs/2305.19245)]| [[Project](https://alteredavatar.github.io/)]||
|HeadSculpt <img src="assets/icon_text.png" width="20" height="20"> |HeadSculpt: Crafting 3D Head Avatars with Text| Neurips 2023| [[Paper](https://arxiv.org/abs/2306.03038)]| [[Project](https://brandonhan.uk/HeadSculpt/)]| [[Github(TBA)](https://github.com/BrandonHanx/HeadSculpt)] |
|  StyleMesh <img src="assets/icon_image.png" width="20" height="20">| StyleMesh: Style Transfer for Indoor 3D Scene Reconstructions   | CVPR 2022 |  [[Paper](https://arxiv.org/abs/2112.01530)]  |  [[Project](https://lukashoel.github.io/stylemesh/)]  | [[Github](https://github.com/lukasHoel/stylemesh)]   | 
|Latent-NeRF  <img src="assets/icon_text.png" width="20" height="20"> |Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures | CVPR 2022 | [[Paper](https://arxiv.org/abs/2211.07600)] |[[Video](https://youtu.be/WwOXzWvGNdc)] | [[Github](https://github.com/eladrich/latent-nerf)]|
| Text2Mesh <img src="assets/icon_text.png" width="20" height="20">|Text2Mesh: Text-Driven Neural Stylization for Meshes  |CVPR 2022|[[Paper](https://arxiv.org/abs/2112.03221)]| [[Project](https://threedle.github.io/text2mesh/)] | [[Github](https://github.com/threedle/text2mesh)]  |
| TANGO <img src="assets/icon_text.png" width="20" height="20">|TANGO: Text-driven Photorealistic and Robust 3D Stylization via Lighting Decomposition  |NeurIPS 2022|[[Paper](http://arxiv.org/abs/2210.11277)]| [[Project](https://cyw-3d.github.io/tango/)] | [[Github](https://github.com/Gorilla-Lab-SCUT/tango)]  |
| CLIP-Mesh <img src="assets/icon_text.png" width="20" height="20">|CLIP-Mesh: Generating textured meshes from text using pretrained image-text models  |SIGGRAPH Asia 2022 |[[Paper](https://arxiv.org/abs/2203.13333)]| [[Project](https://www.nasir.lol/clipmesh)] | [[Github](https://github.com/NasirKhalid24/CLIP-Mesh)]  |
| 3DStyleNet <img src="assets/icon_obj.png" width="20" height="20">| 3DStyleNet: Creating 3D Shapes with Geometric and Texture Style | ICCV 2021| [[Paper](https://research.nvidia.com/labs/toronto-ai/3DStyleNet/assets/3dstyle-paper.pdf)] | [[Project](https://research.nvidia.com/labs/toronto-ai/3DStyleNet/)] |  |
| DGTS <img src="assets/icon_obj.png" width="20" height="20">| Deep Geometric Texture Synthesis| SIGGRAPH 2020 | [[Paper](https://arxiv.org/abs/2007.00074)] | [[Project](https://ranahanocka.github.io/geometric-textures/)]| [[Github](https://github.com/amirhertz/geometric-textures)]|
| Paparazzi <img src="assets/icon_image.png" width="20" height="20">| Paparazzi: Surface Editing by way of Multi-View Image Processing | SIGGRAPH Asia 2018 | [[Paper](https://www.dgp.toronto.edu/projects/paparazzi/paparazzi-surface-editing-by-way-of-multi-view-image-processing-siggraph-asia-2018-liu-et-al.pdf)] | [[Project](https://www.dgp.toronto.edu/projects/paparazzi/)] | [[Github](https://github.com/HTDerekLiu/Paparazzi)] |
|  Neural Renderer <img src="assets/icon_image.png" width="20" height="20">  | Neural 3D Mesh Renderer |CVPR 2018|[[Paper](https://arxiv.org/abs/1711.07566)]|[[Project](https://hiroharu-kato.com/assets/downloads/cvpr_2018_poster.pdf)] | [[Github](https://github.com/hiroharu-kato/style_transfer_3d)] |

</details>


<details open>
<summary>
  
**Novel View or Neural Field Stylization**
<div id="table-novel-view-style"></div>
    
</summary>


|  Abbr.  |  Title   | Venue  | Paper  | Project  | Github  |
|  ----  |  ----    | ----  |----   |----  |----  |
|Styl3R <img src="assets/icon_text.png" width="20" height="20">|Styl3R: Instant 3D Stylized Reconstruction for Arbitrary Scenes and Styles |NeurIPS 2025 | [[Paper](https://arxiv.org/abs/2505.21060)] | [[Project](https://nickisdope.github.io/Styl3R/)] | |
|PrEditor3D <img src="assets/icon_text.png" width="20" height="20">|PrEditor3D: Fast and Precise 3D Shape Editing |CVPR 2025 | [[Paper](https://arxiv.org/abs/2412.06592)] | [[Project](https://ziyaerkoc.com/preditor3d/)] | |
|CTRL-D <img src="assets/icon_text.png" width="20" height="20"><img src="assets/icon_image.png" width="20" height="20"> |CTRL-D: Controllable Dynamic 3D Scene Editing with Personalized 2D Diffusion |CVPR 2025 | [[Paper](https://arxiv.org/abs/2412.01792)] | [[Project](https://ihe-kaii.github.io/CTRL-D/)] | [[Github](https://github.com/IHe-KaiI/CTRL-D)]|
|Instruct-4DGS <img src="assets/icon_text.png" width="20" height="20"> | Efficient Dynamic Scene Editing via 4D Gaussian-based Static-Dynamic Separation | CVPR 2025 | [[Paper](https://arxiv.org/abs/2502.02091)] | [[Project](https://hanbyelcho.info/instruct-4dgs/)] |  |
|Perturb-and-Revise <img src="assets/icon_text.png" width="20" height="20"> |Perturb-and-Revise: Flexible 3D Editing with Generative Trajectories | CVPR 2025 | [[Paper](https://arxiv.org/abs/2412.05279)] | [[Project](https://susunghong.github.io/Perturb-and-Revise/)] | [[Github](https://github.com/SusungHong/Perturb-and-Revise)]|
|SGSST <img src="assets/icon_image.png" width="20" height="20">|SGSST: Scaling Gaussian Splatting Style Transfer | CVPR 2025 | [[Paper](https://arxiv.org/abs/2412.03371)]| [[Project](https://www.idpoisson.fr/galerne/sgsst/)] | [[Github](https://github.com/JianlingWANG2021/SGSST)]|
|Morpheus  <img src="assets/icon_text.png" width="20" height="20">  | Morpheus: Text-Driven 3D Gaussian Splat Shape and Color Stylization | CVPR 2025 | [[Paper](https://arxiv.org/abs/2503.02009)] |[[Project](https://nianticlabs.github.io/morpheus/)] | |
| DreamCatalyst <img src="assets/icon_text.png" width="20" height="20"> | DreamCatalyst: Fast and High-Quality 3D Editing via Controlling Editability and Identity Preservation| ICLR 2025 | [[Paper](https://arxiv.org/abs/2407.11394)] | [[Project](https://dream-catalyst.github.io/)] | [[Github](https://github.com/kaist-cvml/DreamCatalyst)]|
|IPDreamer <img src="assets/icon_image.png" width="20" height="20"> |IPDreamer: Appearance-Controllable 3D Object Generation with Complex Image Prompts | ICLR 2025 | [[Paper](https://arxiv.org/abs/2310.05375)]| | [[Github](https://github.com/zengbohan0217/IPDreamer)]|
| ReGS <img src="assets/icon_image.png" width="20" height="20">| ReGS: Reference-based Controllable Scene Stylization with Gaussian Splatting  | NeurIPS 2024 |[[Paper](https://arxiv.org/abs/2407.07220)] | |  |
| WaSt-3D <img src="assets/icon_obj.png" width="20" height="20">| WaSt-3D: Wasserstein-2 Distance for Scene-to-Scene Stylization on 3D Gaussians  | ECCV 2024 |[[Paper](https://arxiv.org/abs/2409.17917)] | [[Project](https://compvis.github.io/wast3d/)]|  |
|LatentEditor <img src="assets/icon_text.png" width="20" height="20"> |LatentEditor: Text Driven Local Editing of 3D Scenes |ECCV 2024|[[Paper](https://arxiv.org/abs/2312.09313)]|[[Project](https://latenteditor.github.io/)]|[[Github(TBA)](https://github.com/umarkhalidAI/LatentEditor)]|
|MIGS <img align=top src="assets/icon_obj.png" width="32" height="32"> |MIGS: Multi-Identity Gaussian Splatting via Tensor Decomposition | ECCV 2024| [[Paper](https://arxiv.org/abs/2407.07284)]| [[Project](https://aggelinacha.github.io/MIGS/)]||
|3DEgo <img src="assets/icon_text.png" width="20" height="20"> |3DEgo: 3D Editing on the Go!|ECCV 2024| [[Paper](https://arxiv.org/abs/2407.10102)]| [[Project](https://3dego.github.io/)] | [[Data](https://3dego.github.io/)]|
|VcEdit  <img src="assets/icon_text.png" width="20" height="20"> |View-Consistent 3D Editing with Gaussian Splatting| ECCV 2024| [[Paper](https://arxiv.org/abs/2403.11868)]|[[Project](https://yuxuanw.me/vcedit/)]| |
|MALD-NeRF |Taming Latent Diffusion Model for Neural Radiance Field Inpainting| ECCV 2024| [[Paper](https://arxiv.org/abs/2404.09995)]|[[Project](https://hubert0527.github.io/MALD-NeRF/)]||
|DATENeRF <img src="assets/icon_text.png" width="20" height="20">|DATENeRF: Depth-Aware Text-based Editing of NeRFs |ECCV 2024| [[Paper](https://arxiv.org/abs/2404.04526)]| [[Project](https://datenerf.github.io/DATENeRF/)]||
|Gaussian Grouping |Gaussian Grouping: Segment and Edit Anything in 3D Scenes| ECCV 2024 | [[Paper](https://arxiv.org/abs/2312.00732)]| [[Project](https://ymq2017.github.io/gaussian-grouping/)] | [[Github](https://github.com/lkeab/gaussian-grouping)]|
| Watch Your Steps<img src="assets/icon_text.png" width="20" height="20">|Watch Your Steps: Local Image and Scene Editing by Text Instructions | ECCV 2024| [[Paper](https://arxiv.org/abs/2308.08947)]|[[Project](https://ashmrz.github.io/WatchYourSteps/)]||
| CE3D <img src="assets/icon_text.png" width="20" height="20"> |Chat-Edit-3D: Interactive 3D Scene Editing via Text Prompts| ECCV 2024| [[Paper](https://arxiv.org/abs/2407.06842)]| [[Project](https://sk-fun.fun/CE3D/)]| [[Github(TBA)](https://github.com/Fangkang515/CE3D)]|
|GaussCtrl <img src="assets/icon_text.png" width="20" height="20"> |GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting Editing| ECCV 2024| [[Paper](https://arxiv.org/abs/2403.08733)]| [[Project](https://gaussctrl.active.vision/)]| [[Github](https://github.com/ActiveVisionLab/gaussctrl)]|
|Free-Editor <img src="assets/icon_text.png" width="20" height="20"><img src="assets/icon_image.png" width="20" height="20"> | Free-Editor: Zero-shot Text-driven 3D Scene Editing | ECCV 2024 | [[Paper](https://arxiv.org/abs/2312.13663)] | [[Project](https://free-editor.github.io/)] | [[Github(TBA)](https://github.com/nazmul-karim170/FreeEditor-Text-to-3D-Scene-Editing)] |
|Freditor  <img src="assets/icon_text.png" width="20" height="20">|Freditor: High-Fidelity and Transferable NeRF Editing by Frequency Decomposition|Arxiv 2024|[[Paper](https://arxiv.org/abs/2404.02514)] |[[Project](https://aigc3d.github.io/freditor/)]||
|StyleGaussian <img src="assets/icon_image.png" width="20" height="20"> | StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting |Arxiv 2024| [[Paper](https://arxiv.org/abs/2403.07807)]|[[Project](https://kunhao-liu.github.io/StyleGaussian/)]|[[Github](https://github.com/Kunhao-Liu/StyleGaussian)]|
|StylizedGS <img src="assets/icon_image.png" width="20" height="20"> |StylizedGS: Controllable Stylization for 3D Gaussian Splatting| Arxiv 2024| [[Paper](https://arxiv.org/abs/2404.05220)]| | |
|3DFaceHybrid <img src="assets/icon_image.png" width="20" height="20"> | 3D Face Style Transfer with a Hybrid Solution of NeRF and Mesh Rasterization | WACV 2024 | [[Paper](https://openaccess.thecvf.com/content/WACV2024/html/Feng_3D_Face_Style_Transfer_With_a_Hybrid_Solution_of_NeRF_WACV_2024_paper.html)] | |  |
|NeRFEditor <img src="assets/icon_text.png" width="20" height="20"><img src="assets/icon_image.png" width="20" height="20"> | NeRFEditor: Differentiable Style Decomposition for 3D Scene Editing| WACV 2024 | [[Paper](https://arxiv.org/abs/2212.03848)] | [[Project](https://chuny1.github.io/NeRFEditor/nerfeditor.html)] |  |
|TIP-Editor | TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And Image-Prompts | SIGGRAPH/TOG 2024| [[Paper](https://arxiv.org/abs/2401.14828)]| [[Project](https://zjy526223908.github.io/TIP-Editor/)]| [[Github](https://github.com/zjy526223908/TIP-Editor)]|
|Portrait3D <img src="assets/icon_text.png" width="20" height="20">|Portrait3D: Text-Guided High-Quality 3D Portrait Generation Using Pyramid Representation and GANs Prior|SIGGRAPH/TOG 2024|[[Paper](https://arxiv.org/abs/2404.10394)]|[[Project](https://onethousandwu.com/portrait3d.github.io/)]|[[Github](https://github.com/oneThousand1000/Portrait3D)]|
|SketchDream  <img src="assets/icon_text.png" width="20" height="20"><img src="assets/icon_image.png" width="20" height="20">|SketchDream: Sketch-based Text-to-3D Generation and Editing|SIGGRAPH/TOG 2024|[[Paper](https://arxiv.org/abs/2405.06461)]|[[Project](https://geometrylearning.com/SketchDream/)]|[[Github(TBA)](https://github.com/IGLICT/SketchDream/)]|
|DreamFont3D <img src="assets/icon_text.png" width="20" height="20">|DreamFont3D: Personalized Text-to-3D Artistic Font Generation|SIGGRAPH 2024 Conf Paper| [[Paper](https://dl.acm.org/doi/10.1145/3641519.3657476)]|||
|Spice-E <img src="assets/icon_text.png" width="20" height="20"><img src="assets/icon_obj.png" width="20" height="20">|Spice-E: Structural Priors in 3D Diffusion using Cross-Entity Attention|SIGGRAPH 2024 Conf Paper|[[Paper](https://arxiv.org/abs/2311.17834)]|[[Project](https://tau-vailab.github.io/Spice-E/)]|[[Github](https://github.com/TAU-VAILab/Spice-E)]|
|3Doodle <img src="assets/icon_image.png" width="20" height="20">|3Doodle: Compact Abstraction of Objects with 3D Strokes| SIGGRAPH/TOG 2024|[[Paper](https://arxiv.org/abs/2402.03690)]|| 
|ED-NeRF <img src="assets/icon_text.png" width="20" height="20">|ED-NeRF: Efficient Text-Guided Editing of 3D Scene With Latent Space NeRF |ICLR 2024 |[[Paper](https://openreview.net/forum?id=9DvDRTTdlu)] | [[Project](https://ed-nerf.github.io/)] |  |
|FPRF <img src="assets/icon_image.png" width="20" height="20">| FPRF: Feed-Forward Photorealistic Style Transfer of Large-Scale 3D Neural Radiance Fields | AAAI 2024 | [[Paper](https://arxiv.org/abs/2401.05516)] | [[Project](https://kim-geonu.github.io/FPRF/)] | [[Github](https://github.com/postech-ami/FPRF)] |
|PNeSM <img src="assets/icon_image.png" width="20" height="20">|PNeSM: Arbitrary 3D Scene Stylization via Prompt-Based Neural Style Mapping| AAAI 2024 | [[Paper](https://arxiv.org/abs/2403.08252)] | | |
|Neural3DStrokes <br><img src="assets/icon_obj.png" width="20" height="20">(3D Strokes)|Neural 3D Strokes: Creating Stylized 3D Scenes with Vectorized 3D Strokes|CVPR 2024| [[Paper](https://arxiv.org/abs/2311.15637)]| [[Project](https://buaavrcg.github.io/Neural3DStrokes/)]| [[Github](https://github.com/buaavrcg/Neural3DStrokes)]|
|Geo-SRF <img src="assets/icon_image.png" width="20" height="20">|Geometry Transfer for Stylizing Radiance Fields| CVPR 2024| [[Paper]](https://arxiv.org/abs/2402.00863)|[[Project](https://hyblue.github.io/geo-srf/)]|[[Github(TBA)](https://github.com/hyBlue/Geo-SRF)]|
|S-DyRF <img src="assets/icon_image.png" width="20" height="20">|S-DyRF: Reference-Based Stylized Radiance Fields for Dynamic Scenes|CVPR 2024| [[Paper](https://arxiv.org/abs/2403.06205)]| [[Project](https://xingyi-li.github.io/s-dyrf/)]| [[Github](https://github.com/xingyi-li/s-dyrf)]|
|LAENeRF <img src="assets/icon_image.png" width="20" height="20">|LAENeRF: Local Appearance Editing for Neural Radiance Fields| CVPR 2024 |[[Paper](https://arxiv.org/abs/2312.09913)]|[[Project](https://r4dl.github.io/LAENeRF/)]|[[Github](https://github.com/r4dl/LAENeRF)]|
|PDS <img src="assets/icon_text.png" width="20" height="20"> |Posterior Distillation Sampling| CVPR 2024|[[Paper](https://posterior-distillation-sampling.github.io/static/assets/pds.pdf)]|[[Project](https://posterior-distillation-sampling.github.io/)]| [[Github](https://github.com/KAIST-Visual-AI-Group/PDS)]|
|NeRF Analogies <img src="assets/icon_obj.png" width="20" height="20">|NeRF Analogies: Example-Based Visual Attribute Transfer for NeRFs|CVPR 2024|[[Paper](https://arxiv.org/abs/2402.08622)]|[[Project](https://mfischer-ucl.github.io/nerf_analogies/)]||
|UAV-ENeRF  <img src="assets/icon_text.png" width="20" height="20"> |UAV-ENeRF: Text-driven UAV Scene Editing with Neural Radiance Fields|TGRS 2024|[[Paper](https://ieeexplore.ieee.org/document/10476501)]|||
|SC-GS|SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes| CVPR 2024| [[Paper](https://yihua7.github.io/SC-GS-web/materials/SC_GS_Arxiv.pdf)]| [[Project](https://yihua7.github.io/SC-GS-web/)] |[[Github](https://github.com/yihua7/SC-GS)]|
|CineTransfer <img src="assets/icon_image.png" width="20" height="20">|Cinematic Behavior Transfer via NeRF-based Differentiable Filming |CVPR 2024| [[Paper](https://arxiv.org/abs/2311.17754)] | [[Project](https://virtualfilmstudio.github.io/projects/cinetransfer/)] | [[Github](https://github.com/VirtualFilmStudio/Cinetransfer)]|
|SC4D <img src="assets/icon_obj.png" width="20" height="20">|SC4D: Sparse-Controlled Video-to-4D Generation and Motion Transfer| Arxiv 2024| [[Paper](https://arxiv.org/abs/2404.03736)]|[[Project](https://sc4d.github.io/)]||
|GSDeformer|GSDeformer: Direct Cage-based Deformation for 3D Gaussian Splatting|Arxiv 2024| [[Paper](https://arxiv.org/abs/2405.15491)]| [[Project](https://jhuangbu.github.io/gsdeformer/)]||
|IReNe |IReNe: Instant Recoloring of Neural Radiance Fields| CVPR 2024 |[[Paper](https://arxiv.org/abs/2405.19876)]|[[Project](https://iviazz97.github.io/irene/)]||
|Control4D <img src="assets/icon_text.png" width="20" height="20"> |Control4D: Efficient 4D Portrait Editing with Text | CVPR 2024 | [[Paper](https://arxiv.org/abs/2305.20082)] | [[Project](https://control4darxiv.github.io/)] | |
|Instruct 4D-to-4D <img src="assets/icon_text.png" width="20" height="20">|Instruct 4D-to-4D: Editing 4D Scenes as Pseudo-3D Scenes Using 2D Diffusion| CVPR 2024|[[Paper](https://arxiv.org/abs/2406.09402)]|[[Project](https://immortalco.github.io/Instruct-4D-to-4D/)]| |
|GaussianEditor(Huawei) <img src="assets/icon_text.png" width="20" height="20"> | GaussianEditor: Editing 3D Gaussians Delicately with Text Instructions| CVPR 2024| [[Paper](https://arxiv.org/abs/2311.16037)]| [[Project](https://gaussianeditor.github.io/)] |  |
|GaussianEditor(NTU) <img src="assets/icon_text.png" width="20" height="20"> | GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting| CVPR 2024| [[Paper](https://arxiv.org/abs/2311.14521)]| [[Project](https://buaacyw.github.io/gaussian-editor/)] | [[Github](https://github.com/buaacyw/GaussianEditor)] |
|CoARF <img src="assets/icon_image.png" width="20" height="20"> | CoARF: Controllable 3D Artistic Style Transfer for Radiance Fields| 3DV 2024| [[Paper](https://studios.disneyresearch.com/app/uploads/2024/03/CoARF_paper.pdf)]| [[Project](https://studios.disneyresearch.com/2024/03/18/coarf-controllable-3d-artistic-style-transfer-for-radiance-fields/)]|  |
|RePaint-NeRF <img src="assets/icon_text.png" width="20" height="20">|RePaint-NeRF: NeRF Editing via Semantic Masks and Diffusion Models| IJCAI 2023 |[[Paper](https://arxiv.org/abs/2306.05668)]|[[Project](https://starstesla.github.io/repaintnerf/)]| [[Github](https://github.com/StarsTesla/RePaint-NeRF)]|
|DreamEditor <img src="assets/icon_text.png" width="20" height="20"> | DreamEditor: Text-Driven 3D Scene Editing with Neural Fields| SIGGRAPH Asia 2023 Conf Paper| [[Paper](https://arxiv.org/abs/2306.13455)]| [[Project](https://www.sysu-hcp.net/projects/cv/111.html)] | [[Github](https://github.com/zjy526223908/DreamEditor)]|
|AvatarStudio <img src="assets/icon_text.png" width="20" height="20">|AvatarStudio: Text-driven Editing of 3D Dynamic Human Head Avatars|  SIGGRAPH Asia 2023 Conf Paper| [[Paper](https://dl.acm.org/doi/pdf/10.1145/3618368)]|[[Project](https://vcai.mpi-inf.mpg.de/projects/AvatarStudio/)]|
| UPST-NeRF <img src="assets/icon_image.png" width="20" height="20">| UPST-NeRF: Universal Photorealistic Style Transfer of Neural Radiance Fields for 3D Scene | TVCG 2024 | [[Paper](https://ieeexplore.ieee.org/document/10475593)] | [[Project](https://semchan.github.io/UPST_NeRF/)] | [[Github](https://github.com/semchan/UPST-NeRF)] |
|ViCA-NeRF <img src="assets/icon_text.png" width="20" height="20">| ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields|NeurIPS 2023 | [[Paper](https://arxiv.org/abs/2402.00864)]| [[Project](https://dongjiahua.github.io/VICA-NeRF/)]| [[Github](https://github.com/Dongjiahua/VICA-NeRF)] |
| TSNeRF <img src="assets/icon_text.png" width="20" height="20">| TSNeRF: Text-driven stylized neural radiance fields via semantic contrastive learning | Computers & Graphics 2023,  CCF CAD/Graphics 2023 | [[Paper](https://www.sciencedirect.com/science/article/pii/S0097849323001796)] | |  |
| LocalStyleNeRF <img src="assets/icon_image.png" width="20" height="20">| Locally Stylized Neural Radiance Fields | ICCV 2023 |[[Paper](https://arxiv.org/abs/2309.10684)] | [[Project](https://nerfstyle.hkustvgd.com/)] | [[Github](https://github.com/hkust-vgd/nerfstyle)] |
|Instruct-NeRF2NeRF <img src="assets/icon_text.png" width="20" height="20"> | Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions | ICCV 2023|[[Paper](https://arxiv.org/abs/2303.12789)] |[[Project](https://instruct-nerf2nerf.github.io/)]  | [[Github](https://github.com/ayaanzhaque/instruct-nerf2nerf)] |
| Blending-NeRF <img src="assets/icon_text.png" width="20" height="20">| Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields | ICCV 2023 | [[Paper](https://arxiv.org/pdf/2308.11974)] | [[Project](https://seokhunchoi.github.io/Blending-NeRF/)] |  |
|Vox-E <img src="assets/icon_text.png" width="20" height="20">|Vox-E: Text-guided Voxel Editing of 3D Objects |ICCV 2023 |[[Paper](https://arxiv.org/abs/2303.12048/)]|[[Project](https://tau-vailab.github.io/Vox-E/)]|[[Github](https://github.com/TAU-VAILab/Vox-E)]|
|ClimateNeRF |ClimateNeRF: Extreme Weather Synthesis in Neural Radiance Field| ICCV 2023| [[Paper](https://arxiv.org/abs/2211.13226)]| [[Project](https://climatenerf.github.io/)]| [[Github](https://github.com/y-u-a-n-l-i/Climate_NeRF)]|
| LipRF <img src="assets/icon_image.png" width="20" height="20">| Transforming Radiance Field with Lipschitz Network for Photorealistic 3D Scene Stylization |CVPR 2023 |[[Paper](https://arxiv.org/abs/2303.13232)] | |  |
|StyleRF  <img src="assets/icon_image.png" width="20" height="20">| StyleRF: Zero-shot 3D Style Transfer of Neural Radiance Fields | CVPR 2023|[[Paper](https://arxiv.org/abs/2303.10598)]|[[Project](https://kunhao-liu.github.io/StyleRF/)] | [[Github](https://github.com/Kunhao-Liu/StyleRF)] |
| Ref-NPR <img src="assets/icon_image.png" width="20" height="20">| Ref-NPR: Reference-Based Non-Photorealistic Radiance Fields for Controllable Scene Stylization | CVPR 2023|[[Paper](https://arxiv.org/abs/2212.02766)]|[[Project](https://ref-npr.github.io/)] |  [[Github](https://github.com/dvlab-research/Ref-NPR/)] |
| SINE <img src="assets/icon_image.png" width="20" height="20"><img src="assets/icon_text.png" width="20" height="20">|Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field  | CVPR 2023|[[Paper](https://arxiv.org/abs/2303.13277)]|[[Project](https://zju3dv.github.io/sine/)] |  [[Github](https://github.com/zju3dv/SINE)]|
| NeRF-Art <img src="assets/icon_text.png" width="20" height="20">| NeRF-Art: Text-Driven Neural Radiance Fields Stylization | TVCG 2023|[[Paper](https://arxiv.org/abs/2212.08070)]| [[Project](https://cassiepython.github.io/nerfart/)]|  [[Github](https://github.com/cassiePython/NeRF-Art)]|
| InstantNeRFStyle <img src="assets/icon_image.png" width="20" height="20"> | Instant Photorealistic Neural Radiance Fields Stylization | ICASSP 2024|[[Paper](https://arxiv.org/abs/2303.16884)]<br>[[IEEE](https://ieeexplore.ieee.org/abstract/document/10446082)]| | [[Github](https://github.com/lsx0101/Instant-NeRF-Stylization)] |
| SNeRF <img src="assets/icon_image.png" width="20" height="20"> |SNeRF: Stylized Neural Implicit Representations for 3D Scenes  | SIGGRAPH 2022|[[Paper](https://arxiv.org/abs/2207.02363)]| [[Project](https://research.facebook.com/publications/snerf-stylized-neural-implicit-representations-for-3d-scenes/)] |  |
| ArtNV <img src="assets/icon_image.png" width="20" height="20">|Artistic Style Novel View Synthesis Based on A Single Image  |CVPRW 2022, SIGGRAPH Posters 2022 |[[Paper](https://openaccess.thecvf.com/content/CVPR2022W/CVFAD/papers/Tseng_Artistic_Style_Novel_View_Synthesis_Based_on_a_Single_Image_CVPRW_2022_paper.pdf)]|[[Project](https://kuan-wei-tseng.github.io/ArtNV)]  |[[Github](https://github.com/Kuan-Wei-Tseng/ArtNV)]  |
| StylizedNeRF <img src="assets/icon_image.png" width="20" height="20"> | StylizedNeRF: Consistent 3D Scene Stylization as Stylized NeRF via 2D-3D Mutual Learning | CVPR 2022|[[Paper](https://arxiv.org/abs/2205.12183)]| [[Project](http://geometrylearning.com/StylizedNeRF/)]| [[Github](https://github.com/IGLICT/StylizedNeRF)] |
|3D Photo Stylization <img src="assets/icon_image.png" width="20" height="20">| 3D Photo Stylization: Learning to Generate Stylized Novel Views from a Single Image | CVPR 2022|[[Paper](https://arxiv.org/abs/2112.00169)]|[[Project](https://pages.cs.wisc.edu/~fmu/style3d/)]  | [[Github](https://github.com/fmu2/3d_photo_stylization)]  |
| ARF <img src="assets/icon_image.png" width="20" height="20">|ARF: Artistic Radiance Fields  | ECCV 2022|[[Paper](https://arxiv.org/abs/2206.06360)]|[[Project](https://www.cs.cornell.edu/projects/arf/)] | [[Github](https://github.com/Kai-46/ARF-svox2)] |
| INS <img src="assets/icon_image.png" width="20" height="20"> | Unified Implicit Neural Stylization |ECCV 2022 |[[Paper](https://arxiv.org/abs/2204.01943)]|[[Project](https://zhiwenfan.github.io/INS/)]  | [[Github](https://github.com/VITA-Group/INS)] |
| HyperStyle <img src="assets/icon_image.png" width="20" height="20"> | Stylizing 3D Scene via Implicit Representation and HyperNetwork |WACV 2022 | [[Paper](https://arxiv.org/abs/2105.13016)] |  [[Project](https://ztex08010518.github.io/3dstyletransfer/)] | [[Github](https://github.com/ztex08010518/Stylizing-3D-Scene)] |
|  LSNV <img src="assets/icon_image.png" width="20" height="20"> | Learning to stylize novel views |  ICCV 2021  | [[Paper](https://arxiv.org/abs/2105.13509)]   | [[Project](https://hhsinping.github.io/3d_scene_stylization/)]   |  [[Github](https://github.com/hhsinping/stylescene)] |
</details>

<details open>
<summary>

**Multi-view / Light Field Stylization**
<div id="table-light-field-style"></div>     
</summary>

|  Abbr.  |  Title   | Venue  | Paper  | Project  | Github  |
|  ----  |  ----    | ----  |----   |----  |----  |
|MuVieCAST <img src="assets/icon_image.png" width="20" height="20">| MuVieCAST: Multi-View Consistent Artistic Style Transfer|3DV 2024 |[[Paper](https://arxiv.org/abs/2312.05046)]|[[Project](https://muviecast.github.io/)]|[[Github](https://github.com/Mirmix/muviecast)]|
|LFST <img src="assets/icon_image.png" width="20" height="20">|Style Transfer for Light Field Photography | WACV 2020|[[Paper](https://arxiv.org/abs/2002.11220)] ||[[Github](https://github.com/davidmhart/LightFieldStyleTransfer)]
|NSIST <img src="assets/icon_image.png" width="20" height="20"> |Neural Stereoscopic Image Style Transfer |ECCV 2018|[[Paper](https://arxiv.org/abs/1802.09985)]|||
|StereoNST <img src="assets/icon_image.png" width="20" height="20">|Stereoscopic Neural Style Transfer| CVPR 2018| [[Paper](https://arxiv.org/abs/1802.10591)]|[[Video](https://youtu.be/7py0Nq8TxYs)]||
<!--| | | | | |  | -->
<!--| |**Text-driven Stylization** | | | |  |-->
<!--|MVIP-NeRF|MVIP-NeRF: Multi-view 3D Inpainting on NeRF Scenes via Diffusion Prior| CVPR 2024||||-->
</details>


<details open>
<summary>
  
**Point Cloud Stylization**
<div id="table-point-style"></div>
    
</summary>

|  Abbr.  |  Title   | Venue  | Paper  | Project  | Github  |
|  ----  |  ----    | ----  |----   |----  |----  |
| PointInverter | PointInverter: Point Cloud Reconstruction and Editing via a Generative Model with Shape Priors | WACV 2023 | [[Paper](https://arxiv.org/abs/2211.08702)]| [[Project](https://ja-yeon-kim.github.io/PointInverter/)] | |
| PSNet <img src="assets/icon_obj.png" width="20" height="20"><img src="assets/icon_image.png" width="20" height="20">| PSNet: A Style Transfer Network for Point Cloud Stylization on Geometry and Color |WACV 2020 | [[Paper](https://openaccess.thecvf.com/content_WACV_2020/html/Cao_PSNet_A_Style_Transfer_Network_for_Point_Cloud_Stylization_on_WACV_2020_paper.html)]| [[Video](https://www.youtube.com/watch?v=EbUOg1gVcFw&t=1818s)] | [[Github](https://github.com/xucao-42/psnet)] |
</details>


<details open>
<summary>
  
**Volumetric Stylization**
<div id="table-volume-style"></div>
    
</summary>

|  Abbr.  |  Title   | Venue  | Paper  | Project  | Github  |
|  ----  |  ----    | ----  |----   |----  |----  |
|StyleRF-VolVis <img src="assets/icon_image.png" width="20" height="20"> |StyleRF-VolVis: Style Transfer of Neural Radiance Fields for Expressive Volume Visualization| IEEE VIS 2024 |[[Paper](https://arxiv.org/abs/2408.00150)]| | |
| ShaDDR <img src="assets/icon_obj.png" width="20" height="20">  | ShaDDR: Interactive Example-Based Geometry and Texture Generation via 3D Shape Detailization and Differentiable Rendering   | SIGGRAPH Asia 2023 Conf Paper |[[Paper](https://arxiv.org/abs/2306.04889)]  | [[Project](https://qiminchen.github.io/shaddr/)] |[[Github](https://github.com/qiminchen/ShaDDR)] |
| ENST <img src="assets/icon_image.png" width="20" height="20">| Efficient Neural Style Transfer For Volumetric Simulations | SIGGRAPH Asia 2022|  [[Paper](https://studios.disneyresearch.com/app/uploads/2022/10/Efficient-Neural-Style-Transfer-For-Volumetric-Simulations.pdf)]| [[Project](https://studios.disneyresearch.com/2022/11/30/efficient-neural-style-transfer-for-volumetric-simulations/)] |  |
| SKPN <img src="assets/icon_image.png" width="20" height="20">| Volumetric appearance stylization with stylizing kernel prediction network|SIGGRAPH 2021  | [[Paper](https://sites.cs.ucsb.edu/~lingqi/publications/paper_volst.pdf)] | [[Video](https://dl.acm.org/doi/abs/10.1145/3450626.3459799)]|  |
| LNST <img src="assets/icon_image.png" width="20" height="20">| Lagrangian Neural Style Transfer for Fluids | SIGGRAPH 2020 | [[Paper](http://arxiv.org/abs/2005.00803)] | [[Video](https://www.youtube.com/watch?v=WPmUsIVf3-4)]|[[Github](https://github.com/byungsook/neural-flow-style/tree/lnst)]  |
| TNST <img src="assets/icon_image.png" width="20" height="20">|Transport-Based Neural Style Transfer for Smoke Simulations|  SIGGRAPH Asia 2019| [[Paper](http://arxiv.org/abs/1905.07442)] | [[Video](https://www.youtube.com/watch?v=67qVRhoOQPE)]|[[Github](https://github.com/byungsook/neural-flow-style/tree/tnst)]  |

</details>


<details open>
<summary>
  
**Implicit Shape Stylization**
 <div id="table-implicit-shape-style"></div>
    
</summary>

|  Abbr.  |  Title   | Venue  | Paper  | Project  | Github  |
|  ----  |  ----    | ----  |----   |----  |----  |
|NeuSDFusion |NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion, Reconstruction, and Generation|ECCV 2024| [[Paper](https://arxiv.org/abs/2403.18241)]|[[Project](https://weizheliu.github.io/NeuSDFusion/)]||
|CNS-Edit|CNS-Edit: 3D Shape Editing via Coupled Neural Shape Optimization| SIGGRAPH 2024 |[[Paper](https://arxiv.org/abs/2402.02313)]|||
|EXIM <img src="assets/icon_text.png" width="20" height="20">|EXIM: A Hybrid Explicit-Implicit Representation for Text-Guided 3D Shape Generation| SIGGRAPH Asia/TOG 2023| [[Paper](https://arxiv.org/abs/2311.01714)] |[[Project](https://liuzhengzhe.github.io/EXIM.github.io/)] | [[Github](https://github.com/liuzhengzhe/EXIM)] |
|SALAD <img src="assets/icon_text.png" width="20" height="20">|SALAD: Part-Level Latent Diffusion for 3D Shape Generation and Manipulation| ICCV 2023| [[Paper](https://salad3d.github.io/assets/salad.pdf)]| [[Project](https://salad3d.github.io/)]| [[Github](https://github.com/KAIST-Geometric-AI-Group/SALAD)]|
|Neural Wavelet |Neural Wavelet-domain Diffusion for 3D Shape Generation, Inversion, and Manipulation | SIGGRAPH Asia 2022 Conf Paper, TOG 2024| [[Paper](https://arxiv.org/abs/2302.00190)]||[[Github](https://github.com/edward1997104/Wavelet-Generation)]|
|SPAGHETTI |SPAGHETTI: Editing Implicit Shapes Through Part Aware Generation| SIGGRAPH/TOG 2022| [[Paper](https://igl.ethz.ch/projects/SPAGHETTI/SPAGHETTI-paper.pdf)]|[[Project](https://igl.ethz.ch/projects/SPAGHETTI/)]| [[Github](https://github.com/amirhertz/spaghetti)]|

</details>

</details>

<details>
  <summary>
    
## Other Related 3D Generation
  </summary>
  
<div align=left> 
  Image guidance: <img align=top src="assets/icon_image.png" width="32" height="32"> 
  Text guidance: <img align=top src="assets/icon_text.png" width="32" height="32"> 
  3D guidance: <img align=top src="assets/icon_obj.png" width="32" height="32"> 
</div>

|  Abbr.  |  Title   | Venue  | Paper  | Project  | Github  |
|  ----  |  ----    | ----  |----   |----  |----  |
|DiffSplat  <img src="assets/icon_image.png" width="20" height="20"> <img src="assets/icon_text.png" width="20" height="20"> |DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian Splat Generation| ICLR 2025|[[Paper]](https://arxiv.org/abs/2501.16764) |[[Project]](https://chenguolin.github.io/projects/DiffSplat/) | [[Github]](https://github.com/chenguolin/DiffSplat)|
|Meta 3D AssetGen <img src="assets/icon_image.png" width="20" height="20"> <img src="assets/icon_text.png" width="20" height="20"> |Meta 3D AssetGen: Text-to-Mesh Generation with High-Quality Geometry, Texture, and PBR Materials |NeurIPS 2024 |[[Paper](https://assetgen.github.io/static/AssetGen.pdf)]| [[Project](https://assetgen.github.io/)]| |
|WordRobe <img src="assets/icon_text.png" width="20" height="20"> |WordRobe: Text-Guided Generation of Textured 3D Garments| ECCV 2024 | [[Paper](https://arxiv.org/abs/2403.17541)]| [[Project](https://wordrobe24.github.io/WordRobe_Page/)]| | 
|CRM <img src="assets/icon_image.png" width="20" height="20">|CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction Model| ECCV 2024|[[Paper](https://arxiv.org/abs/2403.05034)]||[[Github](https://github.com/thu-ml/CRM)]|
|HiFi-123 <img src="assets/icon_image.png" width="20" height="20">| HiFi-123: Towards High-fidelity One Image to 3D Content Generation | ECCV 2024| [[Paper](https://arxiv.org/abs/2310.06744)]| [[Project](https://drexubery.github.io/HiFi-123/)]| [[Github](https://github.com/AILab-CVC/HiFi-123)]|
|UniDream <img src="assets/icon_text.png" width="20" height="20"> |UniDream: Unifying Diffusion Priors for Relightable Text-to-3D Generation| ECCV 2024 | [[Paper](https://arxiv.org/abs/2312.08754)]| [[Project](https://yg256li.github.io/UniDream/)]| [[Github(TBA)](https://github.com/YG256Li/UniDream)] |
|GVGEN <img src="assets/icon_text.png" width="20" height="20">|GVGEN: Text-to-3D Generation with Volumetric Representation|ECCV 2024| [[Paper](https://arxiv.org/abs/2403.12957)]|[[Project](https://sotamak1r.github.io/gvgen/)]|[[Github](https://github.com/SOTAMak1r/GVGEN)]|
|DreamView <img src="assets/icon_text.png" width="20" height="20">|DreamView: Injecting View-specific Text Guidance into Text-to-3D Generation| ECCV 2024| [[Paper](https://arxiv.org/abs/2404.06119)]||[[Github](https://github.com/iSEE-Laboratory/DreamView)]|
|ScaleDreamer <img src="assets/icon_text.png" width="20" height="20"> |ScaleDreamer: Scalable Text-to-3D Synthesis with Asynchronous Score Distillation|ECCV 2024| [[Paper](https://arxiv.org/abs/2407.02040)]| [[Project](https://sites.google.com/view/scaledreamer-release/)]|[[Github](https://github.com/theEricMa/ScaleDreamer)]|
|RodinHD <img src="assets/icon_image.png" width="20" height="20"> |RodinHD: High-Fidelity 3D Avatar Generation with Diffusion Models|ECCV 2024 | [[Paper](https://arxiv.org/abs/2407.06938)]|[[Project](https://rodinhd.github.io/)]| [[Github(TBA)](https://github.com/RodinHD/RodinHD)]|
|BlockFusion |BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane Extrapolation| SIGGRAPH/TOG 2024|[[Paper](https://arxiv.org/abs/2401.17053)]|[[Project](https://yang-l1.github.io/blockfusion/)]|[[Github(TBA)](https://github.com/Tencent-XR-Vision-Labs/BlockFusion)]|
|MVD<sup>2</sup>  <img src="assets/icon_text.png" width="20" height="20">|MVD<sup>2</sup>: Efficient Multiview 3D Reconstruction for Multiview Diffusion|SIGGRAPH 2024 Conf Paper|[[Paper](https://arxiv.org/abs/2402.14253)]|||
|HiFA <img src="assets/icon_text.png" width="20" height="20">| HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance | ICLR 2024 | [[Paper](https://arxiv.org/abs/2305.18766)]| [[Project](https://josephzhu.com/HiFA-site/)]| [[Github](https://github.com/JunzheJosephZhu/HiFA)]|
| DreamCraft3D <img src="assets/icon_image.png" width="20" height="20"><img src="assets/icon_text.png" width="20" height="20"> | DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior| ICLR 2024| [[Paper](https://arxiv.org/abs/2310.16818)] |[[Project](https://mrtornado24.github.io/DreamCraft3D/)] |  [[Github](https://github.com/deepseek-ai/DreamCraft3D)]| 
| DreamGaussian <img src="assets/icon_text.png" width="20" height="20"> | DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation | ICLR 2024| [[Paper](https://arxiv.org/abs/2309.16653)]| [[Project](https://dreamgaussian.github.io/)]| [[Github](https://github.com/dreamgaussian/dreamgaussian)]|
|Progressive3D <img src="assets/icon_text.png" width="20" height="20"> |Progressive3D: Progressively Local Editing for Text-to-3D Content Creation with Complex Semantic Prompts|ICLR 2024 | [[Paper](https://arxiv.org/abs/2310.11784)]| [[Project](https://cxh0519.github.io/projects/Progressive3D/)]| [[Github](https://github.com/cxh0519/Progressive3D)]|
|SyncDreamer <img src="assets/icon_image.png" width="20" height="20"> |SyncDreamer: Generating Multiview-consistent Images from a Single-view Image| ICLR 2024 | [[Paper](https://arxiv.org/abs/2309.03453)] |[[Project](https://liuyuan-pal.github.io/SyncDreamer/)] |[[Github](https://github.com/liuyuan-pal/SyncDreamer)] |
|GaussianDreamer <img src="assets/icon_text.png" width="20" height="20"> |GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models | CVPR 2024| [[Paper](https://arxiv.org/abs/2310.08529)]| [[Project](https://taoranyi.com/gaussiandreamer/)]| [[Github](https://github.com/hustvl/GaussianDreamer)]|
|GSGEN <img src="assets/icon_text.png" width="20" height="20">|GSGEN: Text-to-3D using Gaussian Splatting| CVPR 2024| [[Paper](https://arxiv.org/abs/2309.16585)]||[[Github](https://github.com/gsgen3d/gsgen)]|
|LucidDreamer <img src="assets/icon_text.png" width="20" height="20"><img src="assets/icon_image.png" width="20" height="20">|LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes| CVPR 2024| [[Paper](https://arxiv.org/abs/2311.13384)]| [[Project](https://luciddreamer-cvlab.github.io/)]| [[Github](https://github.com/luciddreamer-cvlab/LucidDreamer)]|
| DreamFusion <img src="assets/icon_text.png" width="20" height="20">| DreamFusion: Text-to-3D using 2D Diffusion | ICLR 2023 |[[Paper](https://arxiv.org/abs/2209.14988)] | [[Project](https://dreamfusion3d.github.io/)] | [[Github](https://github.com/ashawkey/stable-dreamfusion)]<br>(unofficial) |
|SJC <img src="assets/icon_text.png" width="20" height="20">| Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation | CVPR 2023| [[Paper](https://arxiv.org/abs/2212.00774)]| [[Project](https://pals.ttic.edu/p/score-jacobian-chaining)]| [[Github](https://github.com/pals-ttic/sjc/)]|
|Magic3D <img src="assets/icon_text.png" width="20" height="20">|Magic3D: High-Resolution Text-to-3D Content Creation |CVPR 2023 |[[Paper](https://arxiv.org/abs/2211.10440)]|[[Project](https://research.nvidia.com/labs/dir/magic3d/)]||
|Fantasia3D <img src="assets/icon_text.png" width="20" height="20">|Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation |ICCV 2023 |[[Paper](https://arxiv.org/abs/2303.13873)]|[[Project](https://fantasia3d.github.io/)]|[[Github](https://github.com/Gorilla-Lab-SCUT/Fantasia3D)]|
|Zero-1-to-3 <img src="assets/icon_image.png" width="20" height="20"> |Zero-1-to-3: Zero-shot One Image to 3D Object| ICCV 2023| [[Paper](https://arxiv.org/abs/2303.11328)]|[[Project](https://zero123.cs.columbia.edu/)]| [[Github](https://github.com/cvlab-columbia/zero123)]|
|ProlificDreamer <img src="assets/icon_text.png" width="20" height="20">|ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation |NeurIPS 2023 |[[Paper](https://arxiv.org/abs/2305.16213)]|[[Project](https://ml.cs.tsinghua.edu.cn/prolificdreamer/)]|[[Github](https://github.com/thu-ml/prolificdreamer)]| 
|3D VADER <img src="assets/icon_text.png" width="20" height="20">|Autodecoding Latent 3D Diffusion Models |NeurIPS 2023 |[[Paper](https://arxiv.org/abs/2307.05445)]|[[Project](https://snap-research.github.io/3DVADER/)]|[[Github](https://github.com/snap-research/3DVADER)]|
|CLIP-Mesh <img src="assets/icon_text.png" width="20" height="20">|CLIP-Mesh: Generating textured meshes from text using pretrained image-text models| SIGGRAPH Asia 2022 Conf Paper| [[Paper](https://arxiv.org/abs/2203.13333)]| [[Project](https://www.nasir.lol/clipmesh)]| [[Github](https://github.com/NasirKhalid24/CLIP-Mesh)]|
</details>

<details open>
  <summary>
    
## Other 3D Stylization
  </summary>
  
<div align=left> 
  Image guidance: <img align=top src="assets/icon_image.png" width="32" height="32"> 
  Text guidance: <img align=top src="assets/icon_text.png" width="32" height="32"> 
  3D guidance: <img align=top src="assets/icon_obj.png" width="32" height="32"> 
</div>

|  Abbr.  |  Title   | Venue  | Paper  | Project  | Github  |
|  ----  |  ----    | ----  |----   |----  |----  |
|Texture-GS | Texture-GS: Disentangling the Geometry and Texture for 3D Gaussian Splatting Editing| ECCV 2024 | [[Paper](https://arxiv.org/abs/2403.10050)]| [[Project](https://slothfulxtx.github.io/TexGS/)]| [[Github](https://github.com/slothfulxtx/Texture-GS)]|
|SMooDi <img src="assets/icon_text.png" width="20" height="20">|SMooDi: Stylized Motion Diffusion Model| ECCV 2024| [[Paper](https://arxiv.org/abs/2407.12783)]|[[Project](https://neu-vi.github.io/SMooDi/)]| [[Github](https://github.com/neu-vi/SMooDi)]|
|SRFoE|Stylized Rendering as a Function of Expectation|SIGGRAPH/TOG 2024|[[Paper](http://cv.rexwe.st/pdf/srfoe.pdf)]||
|LayGA <img src="assets/icon_obj.png" width="20" height="20">|LayGA: Layered Gaussian Avatars for Animatable Clothing Transfer|SIGGRAPH 2024 Conf Paper| [[Paper](https://arxiv.org/abs/2405.07319)]| [[Project](https://jsnln.github.io/layga/index.html)]||
|  Stealth Shaper  | Stealth Shaper: Reflectivity Optimization as Surface Stylization    | SIGGRAPH 2023 Conf Paper| [[Paper](https://dl.acm.org/doi/10.1145/3588432.3591542)]  | [[Project](https://kenji-tojo.github.io/publications/stealthshaper/)]  | [[Github](https://github.com/kenji-tojo/stealth-shaper)]  |
|Stroke Transfer <img align=top src="assets/icon_image.png" width="20" height="20">  <img src="assets/icon_obj.png" width="20" height="20">|Stroke Transfer: Example-based Synthesis of Animatable Stroke Styles|SIGGRAPH 2022 Conf Paper| [[Paper](http://www.cg.it.aoyama.ac.jp/yonghao/sig22/StrokeTransfer.pdf)] |[[Project](http://www.cg.it.aoyama.ac.jp/yonghao/sig22/abstsig22.html)]| [[Github](https://github.com/AGU-Graphics/StrokeTransfer)]|
|  Normal-Driven <img src="assets/icon_obj.png" width="20" height="20"> |  Normal‐Driven Spherical Shape Analogies |  Computer Graphics Forum, SGP 2021  | [[Paper](https://arxiv.org/abs/2104.11993)]  | [[Video](https://youtu.be/p4hLJ0xVGcM)] |[[Github1](https://github.com/HTDerekLiu/normal_driven_MATLAB)] <br>[[Github2](https://github.com/HTDerekLiu/normal_driven_cpp)] |
|PBFLR|Physically-based Feature Line Rendering|SIGGRAPH Asia/TOG 2021|[[Paper](http://cv.rexwe.st/pdf/pbflr.pdf)]|[[Project](http://lines.rexwe.st/)]|
| StyleProp <img align=top src="assets/icon_image.png" width="20" height="20">  | StyleProp: Real‐time Example‐based Stylization of 3D Models  | Computer Graphics Forum, PG 2020  | [[Paper](https://dcgi.fel.cvut.cz/home/sykorad/Hauptfleisch20-PG.pdf)]  | [[Project](https://dcgi.fel.cvut.cz/home/sykorad/styleprop.html)]  |  |
| StyleBlit <img align=top src="assets/icon_image.png" width="20" height="20"> |StyleBlit: Fast Example‐Based Stylization with Local Guidance | Computer Graphics Forum, EG 2019 | [[Paper](https://dcgi.fel.cvut.cz/home/sykorad/Sykora19-EG.pdf)]| [[Project](https://dcgi.fel.cvut.cz/home/sykorad/styleblit.html)] | [[Github](https://github.com/jamriska/styleblit)] <br> [[Unity Plugin](https://dcgi.fel.cvut.cz/home/sykorad/StyleBlit/unity_plugin.zip)]|
| Cubic Stylization <img src="assets/icon_obj.png" width="20" height="20">  | Cubic Stylization   | SIGGRAPH Asia 2019  |[[Paper](https://arxiv.org/abs/1910.02926)]  | [[Project](https://www.dgp.toronto.edu/projects/cubic-stylization/)] |[[Github1](https://github.com/HTDerekLiu/CubicStylization_MATLAB)] <br>[[Github2](https://github.com/HTDerekLiu/CubicStylization_Cpp)]  |
| StyleLit <img align=top src="assets/icon_image.png" width="20" height="20"> |StyLit: Illumination-Guided Example-Based Stylization of 3D Renderings | SIGGRAPH 2016 | [[Paper](https://dcgi.fel.cvut.cz/home/sykorad/Fiser16-SIG.pdf)]| [[Project](https://dcgi.fel.cvut.cz/home/sykorad/stylit)] | [[Demo](http://stylit.org/)] |

</details>

## Mesh Stylization
**Neural 3D Mesh Renderer**<br>
*Hiroharu Kato, Yoshitaka Ushiku, Tatsuya Harada*<br>
CVPR 2018. [[Paper](https://arxiv.org/abs/1711.07566)][[Project](https://hiroharu-kato.com/assets/downloads/cvpr_2018_poster.pdf)] [[Github](https://github.com/hiroharu-kato/style_transfer_3d)]  <br>
_Features_:
- approximate gradient for rasterization
- single-image 3d mesh reconstruction with 2d supervision
- gradient-based 3d mesh editing including 2d->3d style transfer(image->mesh)


**Paparazzi: Surface Editing by way of Multi-View Image Processing**<br>
*Hsueh-Ti Derek Liu, Michael Tao, Alec Jacobson*<br>
SIGGRAPH Asia 2018.  [[Paper](https://www.dgp.toronto.edu/projects/paparazzi/paparazzi-surface-editing-by-way-of-multi-view-image-processing-siggraph-asia-2018-liu-et-al.pdf)]  [[Project](https://www.dgp.toronto.edu/projects/paparazzi/)]  [[Github](https://github.com/HTDerekLiu/Paparazzi)]  <br>
_Features_:
- image-guided mesh surface stylization
- vertex position optimization
- sample training cameras on the offset surface

**3DStyleNet: Creating 3D Shapes with Geometric and Texture Style**<br>
*Kangxue Yin, Jun Gao, Maria Shugrina, Sameh Khamis, Sanja Fidler*<br>
ICCV 2021. [[Paper](https://research.nvidia.com/labs/toronto-ai/3DStyleNet/assets/3dstyle-paper.pdf)]  [[Project](https://research.nvidia.com/labs/toronto-ai/3DStyleNet/)]  <br>
_Features_:
- 3D shape and texture style tranfer
- 3D textured model as guidance
- Large 3D data driven

**StyleMesh: Style Transfer for Indoor 3D Scene Reconstructions**<br>
*Lukas Höllein, Justin Johnson, Matthias Nießner*<br>
CVPR 2022. [[Paper](https://arxiv.org/abs/2112.01530)] [[Project](https://lukashoel.github.io/stylemesh/)] [[Github](https://github.com/lukasHoel/stylemesh)]  <br>
_Features_:
- 3D consistent texture optimization
- depth-aware and angle-aware optimization for equally-sized and unstreched stylization patterns

**Text2Mesh: Text-Driven Neural Stylization for Meshes**<br>
*Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, Rana Hanocka*<br>
CVPR 2022.[[Paper](https://arxiv.org/abs/2112.03221)] [[Project](https://threedle.github.io/text2mesh/)] [[Github](https://github.com/threedle/text2mesh)]  <br>
_Features_:
- based on CLIP[[Github](https://github.com/openai/CLIP)]
- handle low quality meshes

**TANGO: Text-driven Photorealistic and Robust 3D Stylization via Lighting Decomposition** <br>
*Yongwei Chen, Rui Chen, Jiabao Lei, Yabin Zhang, Kui Jia*<br>
Neurips 2022. [[Paper](http://arxiv.org/abs/2210.11277)] [[Project](https://cyw-3d.github.io/tango/)] [[Github](https://github.com/Gorilla-Lab-SCUT/tango)] <br>

**Creative Birds: Self-Supervised Single-View 3D Style Transfer**<br>
*Renke Wang, Guimin Que, Shuo Chen, Xiang Li, Jun Li, Jian Yang* <br>
ICCV 2023. [[Paper](https://arxiv.org/abs/2307.14127)] [[Github](https://github.com/wrk226/creative_birds)]<br>

**Text2Scene: Text-Driven Indoor Scene Stylization With Part-Aware Details**<br>
*Inwoo Hwang, Hyeonwoo Kim, Young Min Kim*<br>
CVPR 2023. [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Hwang_Text2Scene_Text-Driven_Indoor_Scene_Stylization_With_Part-Aware_Details_CVPR_2023_paper.html)] [[Video](https://www.youtube.com/watch?v=CGIXY2kwIYM)] [[Github](https://github.com/uvavision/Text2Scene)] <br>

**TextDeformer: Geometry Manipulation using Text Guidance**<br>
*William Gao, Noam Aigerman, Thibault Groueix, Vladimir G. Kim, Rana Hanocka*<br>
SIGGRAPH Conference Proceedings 2023. [[Paper](https://arxiv.org/abs/2304.13348)]  [[Project](https://threedle.github.io/TextDeformer/)] [[Github](https://github.com/threedle/TextDeformer)] <br>

**TEXTure: Text-Guided Texturing of 3D Shapes**<br>
*Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, Daniel Cohen-Or*<br>
SIGGRAPH Conference Proceedings 2023. [[Paper](https://arxiv.org/abs/2302.01721)]  [[Project](https://texturepaper.github.io/TEXTurePaper/)]  [[Github](https://github.com/TEXTurePaper/TEXTurePaper)] <br>

**Text2Tex: Text-driven Texture Synthesis via Diffusion Models**<br>
*Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey Tulyakov, Matthias Nießner*<br>
ICCV 2023. [[Paper](https://daveredrum.github.io/Text2Tex/static/Text2Tex.pdf)]  [[Project](https://daveredrum.github.io/Text2Tex/)]  [[Github](https://github.com/daveredrum/Text2Tex)] <br>

**X-Mesh:Towards Fast and Accurate Text-driven 3D Stylization via Dynamic Textual Guidance**<br>
*Yiwei Ma, Xiaioqing Zhang, Xiaoshuai Sun, Jiayi Ji, Haowei Wang, Guannan Jiang, Weilin Zhuang, Rongrong Ji*<br>
ICCV 2023. [[Paper](https://arxiv.org/abs/2303.15764)]  [[Project](https://xmu-xiaoma666.github.io/Projects/X-Mesh/)]  [[Github](https://github.com/xmu-xiaoma666/X-Mesh)] <br>

## Volumetric Stylization
**Transport-Based Neural Style Transfer for Smoke Simulations**  <br>
*Byungsoo Kim, Vinicius C. Azevedo, Markus Gross, Barbara Solenthaler*<br>
SIGGRAPH Asia 2019.| [[Paper](http://arxiv.org/abs/1905.07442)]  [[Video](https://www.youtube.com/watch?v=67qVRhoOQPE)] [[Github](https://github.com/byungsook/neural-flow-style/tree/tnst)]  <br>
_Features_:
- dynamic smoke
- optimization-based stylization
 
**Lagrangian Neural Style Transfer for Fluids** <br>
*Byungsoo Kim, Vinicius C. Azevedo, Markus Gross, Barbara Solenthaler*<br>
SIGGRAPH 2020. [[Paper](http://arxiv.org/abs/2005.00803)]  [[Video](https://www.youtube.com/watch?v=WPmUsIVf3-4)] [[Github](https://github.com/byungsook/neural-flow-style/tree/lnst)]  <br>
_Features_:
- fluid
- optimization-based stylization
  
**Volumetric appearance stylization with stylizing kernel prediction network** <br>
*Jie Guo, Mengtian Li, Zijing Zong, Yuntao Liu, Jingwu He, Yanwen Guo, Ling-Qi Yan*<br>
SIGGRAPH 2021. [[Paper](https://sites.cs.ucsb.edu/~lingqi/publications/paper_volst.pdf))]  [[Video](https://dl.acm.org/doi/abs/10.1145/3450626.3459799)]|  <br>
_Features_:
- dynamic or static model
- arbitrary style transfer

**Efficient Neural Style Transfer For Volumetric Simulations**<br>
*Joshua Aurand, Raphaël Oritz, Sylvia Nauer, Vinicius Azevedo*<br>
ACM SIGGRAPH Asia, 2022. [[Paper](https://studios.disneyresearch.com/app/uploads/2022/10/Efficient-Neural-Style-Transfer-For-Volumetric-Simulations.pdf)][[Video](https://youtu.be/8tD5Yt3smDw)]<br>
_Features_:
- mainly for smoke
- view-independent stylization
- feed-forward network for acceleration


## Point Cloud Stylization
**PSNet: A Style Transfer Network for Point Cloud Stylization on Geometry and Color**<br>
*Xu Cao, Weimin Wang, Katashi Nagao, Ryosuke Nakamura*<br>
Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2020<br>
[[Paper](https://openaccess.thecvf.com/content_WACV_2020/html/Cao_PSNet_A_Style_Transfer_Network_for_Point_Cloud_Stylization_on_WACV_2020_paper.html)] [[Github](https://github.com/xucao-42/psnet)] [[Video](https://www.youtube.com/watch?v=EbUOg1gVcFw&t=1818s)]
_Features_:
- explicit point cloud stylization
- geometric and color stylization

## Neural Field Stylization
### General Neural Rendering
**Learning to stylize novel views**<br>
*Hsin-Ping Huang, Hung-Yu Tseng, Saurabh Saini, Maneesh Singh, Ming-Hsuan Yang.* <br>
ICCV 2021. [[Paper](https://arxiv.org/abs/2105.13509)] [[Project](https://hhsinping.github.io/3d_scene_stylization/)] [[Github](https://github.com/hhsinping/stylescene)]  <br>
_Features_:
- Point-cloud based feature
- Style feature transformation
- Require MVS reconstruction, depth rendering
- zero-shot novel view stylization

**3D Photo Stylization: Learning to Generate Stylized Novel Views from a Single Image**<br>
*Fangzhou Mu, Jian Wang, Yicheng Wu, Yin Li*<br>
CVPR 2022. [[Paper](https://arxiv.org/abs/2112.00169)] [[Project](https://pages.cs.wisc.edu/~fmu/style3d/)] [[Github](https://github.com/fmu2/3d_photo_stylization)]  <br>
_Features_:
- pre-trained NeRF with color prediction module replaced with a style network
- mutual learning for two parts mentioned above

**Artistic Style Novel View Synthesis Based on A Single Image**<br>
*Kuan-Wei Tseng, Yao-Chih Lee, Chu-Song Chen*<br>
CVPR Workshop 2022 / SIGGRAPH Poster 2022.[[Paper](https://openaccess.thecvf.com/content/CVPR2022W/CVFAD/papers/Tseng_Artistic_Style_Novel_View_Synthesis_Based_on_a_Single_Image_CVPRW_2022_paper.pdf)] [[Project](https://kuan-wei-tseng.github.io/ArtNV)] [[Github](https://github.com/Kuan-Wei-Tseng/ArtNV)] <br>
_Features_:
- based on Synsin(differentiable point cloud renderer)[[Paper](https://arxiv.org/abs/1912.08804)]
- occlusion aware dense matching
- mainly for VR

<!-- CLIP3DStyler Arxiv 2023, point cloud based neural rendering-->

### Radiance Field Based Rendering
**Stylizing 3D Scene via Implicit Representation and HyperNetwork**<br>
*Pei-Ze Chiang, Meng-Shiun Tsai, Hung-Yu Tseng, Wei-sheng Lai, Wei-Chen Chiu.* <br>
WACV 2022. [[Paper](https://arxiv.org/abs/2105.13016)] [[Project](https://ztex08010518.github.io/3dstyletransfer/)] [[Github](https://github.com/ztex08010518/Stylizing-3D-Scene)]  <br>
_Features_:
- hypernetwork to control NeRF weights
- two-step training and patch-subsampling
- zero-shot novel view stylization

**StylizedNeRF: Consistent 3D Scene Stylization as Stylized NeRF via 2D-3D Mutual Learning**<br>
*Yi-Hua Huang, Yue He, Yu-Jie Yuan, Yu-Kun Lai, Lin Gao*<br>
CVPR 2022. [[Paper](https://arxiv.org/abs/2205.12183)] [[Project](http://geometrylearning.com/StylizedNeRF/)] [[Github](https://github.com/IGLICT/StylizedNeRF)]  <br>
_Features_:
- pre-trained NeRF with color prediction module replaced with a style network
- mutual learning for two parts mentioned above
- zero-shot novel view stylization

**ARF: Artistic Radiance Fields**<br>
*Kai Zhang, Nick Kolkin, Sai Bi, Fujun Luan, Zexiang Xu, Eli Shechtman, Noah Snavely*<br>
ECCV 2022. [[Paper](https://arxiv.org/abs/2206.06360)] [[Project](https://www.cs.cornell.edu/projects/arf/)] [[Github](https://github.com/Kai-46/ARF-svox2)]  <br>
_Features_:
- pre-trained NeRF with color prediction module replaced with a style network
- mutual learning for two parts mentioned above

**Unified Implicit Neural Stylization**<br>
*Zhiwen Fan, Yifan Jiang, Peihao Wang, Xinyu Gong, Dejia Xu, Zhangyang Wang*<br>
ECCV 2022. [[Paper](https://arxiv.org/abs/2204.01943)] [[Project](https://zhiwenfan.github.io/INS/)] [[Github](https://github.com/VITA-Group/INS)]  <br>
_Features_:
- continuous in the style space(allow interpolation between styles)

**Instant Neural Radiance Fields Stylization**<br>
*Shaoxu Li, Ye Pan*<br>
[[Paper](https://arxiv.org/abs/2303.16884)] [[Github](https://github.com/lsx0101/Instant-NeRF-Stylization)] <br>
_Features_:
- training for novel view stylization image synthesis only costs 10 minutes.
- extend the style target of NeRF stylization from images to 3D scene image sets

**SNeRF: Stylized Neural Implicit Representations for 3D Scenes**<br>
*Thu Nguyen-Phuoc, Feng Liu, Lei Xiao*<br>
SIGGRAPH 2022.[[Paper](https://arxiv.org/abs/2207.02363)]<br>
_Features_:
- applies to indoor scenes, outdoor scenes and 4D dynamic avatars
- reduce GPU memory requirement

**Transforming Radiance Field with Lipschitz Network for Photorealistic 3D Scene Stylization**<br>
*Zicheng Zhang, Yinglu Liu, Congying Han, Yingwei Pan, Tiande Guo, Ting Yao*<br>
CVPR 2023. [[Paper](https://arxiv.org/abs/2303.13232)] <br>
_Features_:
- photo-realistic
- based on Lipschitz transformation and condition

**StyleRF: Zero-shot 3D Style Transfer of Neural Radiance Fields**<br>
*Kunhao Liu, Fangneng Zhan, Yiwen Chen, Jiahui Zhang, Yingchen Yu, Abdulmotaleb El Saddik, Shijian Lu, Eric Xing*<br>
CVPR 2023. [[Paper](https://arxiv.org/abs/2303.10598)] [[Project](https://kunhao-liu.github.io/StyleRF/)] [[Github](https://github.com/Kunhao-Liu/StyleRF)] <br>
_Features_:
- zero-shot 3d stylization
- sampling-invariant content transformation
- deferring style transformation to 2D feature maps
- zero-shot novel view stylization

**Ref-NPR: Reference-Based Non-Photorealistic Radiance Fields for Controllable Scene Stylization**<br>
*Yuechen Zhang, Zexin He, Jinbo Xing, Xufeng Yao, Jiaya Jia*<br>
CVPR 2023. [[Paper](https://arxiv.org/abs/2212.02766)] [[Project](https://ref-npr.github.io/)] [[Github](https://github.com/dvlab-research/Ref-NPR/)] <br>
_Features_:
- reference based ray registration process and a template-based feature matching scheme to achieve geometrically and perceptually consistent stylization

**Locally Stylized Neural Radiance Fields**<br>
*Hong-Wing Pang, Binh-Son Hua, Sai-Kit Yeung*<br>
ICCV 2023. [[Paper](https://arxiv.org/abs/2309.10684)]  [[Project](https://nerfstyle.hkustvgd.com/)] [[Github](https://github.com/hkust-vgd/nerfstyle)] <br>
_Features_:
- explicitly semantic match stylization
- based on instant-NGP
- room stylization

**Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field**<br>
*Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, Zhaopeng Cui*<br>
CVPR 2023. [[Paper](https://arxiv.org/abs/2303.13277)] [[Project](https://zju3dv.github.io/sine/)] [[Github](https://github.com/zju3dv/SINE)] <br>
_Features_:
- edit a photo-realistic NeRF with a single-view image or with text prompts
- cyclic constraints with a proxy mesh for geometric editing
- the color compositing mechanism to enhance texture editing
- feature-cluster-based regularization to control the affected editing area and maintain irrelevant parts unchanged
- 
**NeRF-Art: Text-Driven Neural Radiance Fields Stylization**<br>
*Can Wang, Ruixiang Jiang, Menglei Chai, Mingming He, Dongdong Chen, Jing Liao*<br>
TVCG 2023. [[Paper](https://arxiv.org/abs/2212.08070)] [[Project](https://cassiepython.github.io/nerfart/)] [[Github](https://github.com/cassiePython/NeRF-Art)] <br>
_Features_:
- combine CLIP[[Github](https://github.com/openai/CLIP)] with NeRF
- cover global structures and local details 
- adopt a weight regularization to effectively reduce cloudy artifacts and geometry noises

**Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions**<br>
*Ayaan Haque, Matthew Tancik, Alexei A. Efros, Aleksander Holynski, Angjoo Kanazawa*<br>
ICCV 2023. [[Paper](https://arxiv.org/abs/2303.12789)] [[Project](https://instruct-nerf2nerf.github.io/)] [[Github](https://github.com/ayaanzhaque/instruct-nerf2nerf)] <br>
_Features_:
- extract shape and appearance priors from a 2D diffusion model
- edit input images and change 3d implicit field representation

**Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields**<br>
ICCV 2023. [[Paper](https://arxiv.org/pdf/2308.11974)]  [[Project](https://seokhunchoi.github.io/Blending-NeRF/)] <br>
_Features_:
- text-guided local editing

<!--[[Paper]()] [[Project]()] [[Github]()] <br>-->

